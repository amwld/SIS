{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TC_DATA_PREP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFLFlo1ZIKog",
        "colab_type": "text"
      },
      "source": [
        "# Toxic Comment Data Preparation\n",
        "Amanda Maiwald, Martin Falli, Radoslav Evtimov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpE2_g0TIRNi",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries, data and setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l58W_buc8U-O",
        "colab_type": "code",
        "outputId": "7688e608-9802-4ddd-b7de-c321689cd79a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6yayGpl--Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Amanda directory structure\n",
        "base_dir = '/content/drive/My Drive/BERT/BERT_Code_Input_Output/'\n",
        "data_dir = base_dir + 'Data/'\n",
        "model_dir = base_dir + 'Model_Output_Keras/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsDJyWEx9NlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8JATVIJgkhQ",
        "colab_type": "text"
      },
      "source": [
        "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
        "\n",
        "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
        "\n",
        "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wguU1O46LXy3",
        "colab_type": "text"
      },
      "source": [
        "## Data\n",
        "### Read in data for training and predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxWtcedgBQx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(data_dir + '/train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MxAPF7hFEDw",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdM9PEkUE1xD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean data completely \n",
        "def substitute_repeats_fixed_len(text, nchars, ntimes=3):\n",
        "    return re.sub(r\"(\\S{{{}}})(\\1{{{},}})\".format(nchars, ntimes-1), r\"\\1\", text)\n",
        "  \n",
        "def substitute_repeats(text, ntimes=3):\n",
        "    for nchars in range(1, 20):\n",
        "        text         = substitute_repeats_fixed_len(text, nchars, ntimes)\n",
        "    return text\n",
        "\n",
        "def text_to_wordlist(text, remove_stop_words=True, stem_words=False, with_punct_sent=False):\n",
        "    stop_words       = set(['a', 'the', \"an\", \"are\", \"as\",  'did',\n",
        "                       \"do\", \"is\", \"has\", \"have\", \"had\", \"was\", \"were\",\n",
        "                       \"will\", \"would\", \"am\", \"it\", \"for\", \"on\", \"it\", \"of\"])\n",
        "    #from string import punctuation\n",
        "    import re\n",
        "    NEG_CONTRACTIONS    = [\n",
        "                          (r'aren\\'t', 'are not'),\n",
        "                          (r'can\\'t', 'can not'),\n",
        "                          (r'couldn\\'t', 'could not'),\n",
        "                          (r'daren\\'t', 'dare not'),\n",
        "                          (r'didn\\'t', 'did not'),\n",
        "                          (r'doesn\\'t', 'does not'),\n",
        "                          (r'don\\'t', 'do not'),\n",
        "                          (r'isn\\'t', 'is not'),\n",
        "                          (r'hasn\\'t', 'has not'),\n",
        "                          (r'haven\\'t', 'have not'),\n",
        "                          (r'hadn\\'t', 'had not'),\n",
        "                          (r'mayn\\'t', 'may not'),\n",
        "                          (r'mightn\\'t', 'might not'),\n",
        "                          (r'mustn\\'t', 'must not'),\n",
        "                          (r'needn\\'t', 'need not'),\n",
        "                          (r'oughtn\\'t', 'ought not'),\n",
        "                          (r'shan\\'t', 'shall not'),\n",
        "                          (r'shouldn\\'t', 'should not'),\n",
        "                          (r'wasn\\'t', 'was not'),\n",
        "                          (r'weren\\'t', 'were not'),\n",
        "                          (r'won\\'t', 'will not'),\n",
        "                          (r'wouldn\\'t', 'would not'),\n",
        "                          (r'ain\\'t', 'am not') # not only but stopword anyway\n",
        "                          ]\n",
        "    OTHER_CONTRACTIONS = [\n",
        "                          #(r\"'m\", 'am'),\n",
        "                          (r\"'ll\", ' will'),\n",
        "                          (r\"'s\", ' has'), # or 'is' but both are stopwords\n",
        "                          (r\"'d\", ' had'), # or 'would' but both are stopwords\n",
        "                          (r\"'ve\", \" have\"),\n",
        "                           (r\"'re\", \" are\")   \n",
        "    ]\n",
        "    OTHER_RPS          = [\n",
        "                          (\"&lt;3\", \" good \"),\n",
        "                          (\":d\", \" good \"),\n",
        "                          (\":dd\", \" good \"),\n",
        "                          (\":p\", \" good \"),\n",
        "                          (\"8)\", \" good \"),\n",
        "                          (\":-)\", \" good \"),\n",
        "                          (\":)\", \" good \"),\n",
        "                          (\";)\", \" good \"),\n",
        "                          (\"(-:\", \" good \"),\n",
        "                          (\"(:\", \" good \"),\n",
        "                          (\"yay!\", \" good \"),\n",
        "                          (\"yay\", \" good \"),\n",
        "                          (\"yaay\", \" good \"),\n",
        "                          (\"yaaay\", \" good \"),\n",
        "                          (\"yaaaay\", \" good \"),\n",
        "                          (\"yaaaaay\", \" good \"),\n",
        "                          (\":/\", \" bad \"),\n",
        "                          (\":&gt;\", \" sad \"),\n",
        "                          (\":')\", \" sad \"),\n",
        "                          (\":-(\", \" bad \"),\n",
        "                          (\":(\", \" bad \"),\n",
        "                          (\":s\", \" bad \"),\n",
        "                          (\":-s\", \" bad \"),\n",
        "                          (\"&lt;3\", \" heart \"),\n",
        "                          (\":d\", \" smile \"),\n",
        "                          (\":p\", \" smile \"),\n",
        "                          (\":dd\", \" smile \"),\n",
        "                          (\"8)\", \" smile \"),\n",
        "                          (\":-)\", \" smile \"),\n",
        "                          (\":)\", \" smile \"),\n",
        "                          (\";)\", \" smile \"),\n",
        "                          (\"(-:\", \" smile \"),\n",
        "                          (\"(:\", \" smile \"),\n",
        "                          (\":/\", \" worry \"),\n",
        "                          (\":&gt;\", \" angry \"),\n",
        "                          (\":')\", \" sad \"),\n",
        "                          (\":-(\", \" sad \"),\n",
        "                          (\":(\", \" sad \"),\n",
        "                          (\":s\", \" sad \"),\n",
        "                          (\":-s\", \" sad \"),\n",
        "                          (r\"\\br\\b\", \"are\"),\n",
        "                          (r\"\\bu\\b\", \"you\"),\n",
        "                          (r\"\\bhaha\\b\", \"ha\"),\n",
        "                          (r\"\\bhahaha\\b\", \"ha\"),\n",
        "                          (r\"\\bdon't\\b\", \"do not\"),\n",
        "                          (r\"\\bdoesn't\\b\", \"does not\"),\n",
        "                          (r\"\\bdidn't\\b\", \"did not\"),\n",
        "                          (r\"\\bhasn't\\b\", \"has not\"),\n",
        "                          (r\"\\bhaven't\\b\", \"have not\"),\n",
        "                          (r\"\\bhadn't\\b\", \"had not\"),\n",
        "                          (r\"\\bwon't\\b\", \"will not\"),\n",
        "                          (r\"\\bwouldn't\\b\", \"would not\"),\n",
        "                          (r\"\\bcan't\\b\", \"can not\"),\n",
        "                          (r\"\\bcannot\\b\", \"can not\"),\n",
        "                          (r\"\\bi'm\\b\", \"i am\"),\n",
        "                          (\"m\", \"am\"),\n",
        "                          (\"r\", \"are\"),\n",
        "                          (\"u\", \"you\"),\n",
        "                          (\"haha\", \"ha\"),\n",
        "                          (\"hahaha\", \"ha\"),\n",
        "                          (\"don't\", \"do not\"),\n",
        "                          (\"doesn't\", \"does not\"),\n",
        "                          (\"didn't\", \"did not\"),\n",
        "                          (\"hasn't\", \"has not\"),\n",
        "                          (\"haven't\", \"have not\"),\n",
        "                          (\"hadn't\", \"had not\"),\n",
        "                          (\"won't\", \"will not\"),\n",
        "                          (\"wouldn't\", \"would not\"),\n",
        "                          (\"can't\", \"can not\"),\n",
        "                          (\"cannot\", \"can not\"),\n",
        "                          (\"i'm\", \"i am\"),\n",
        "                          (\"m\", \"am\"),\n",
        "                          (\"i'll\" , \"i will\"),\n",
        "                          (\"its\" , \"it is\"),\n",
        "                          (\"it's\" , \"it is\"),\n",
        "                          (\"'s\" , \" is\"),\n",
        "                          (\"that's\" , \"that is\"),\n",
        "                          (\"weren't\" , \"were not\")\n",
        "    ]\n",
        "    \n",
        "    # Clean the text, with the option to remove stop_words and to stem words.\n",
        "    text                = text.lower()\n",
        "    \n",
        "    for t in NEG_CONTRACTIONS:\n",
        "            text        = re.sub(t[0], t[1], text)\n",
        "\n",
        "\n",
        "    for t in OTHER_CONTRACTIONS:\n",
        "            text        = re.sub(t[0], t[1], text)\n",
        "    for t in OTHER_RPS:\n",
        "            #print(t)\n",
        "            text.replace(t[0], t[1])\n",
        "            #text = re.sub(t[0], t[1], text)    \n",
        "    \n",
        "    # Clean the text\n",
        "    if with_punct_sent:\n",
        "      text              = re.sub(r\"[^A-Za-z0-9!.?]\", \" \", text)\n",
        "    else: \n",
        "      text              = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
        "    \n",
        "    text                = re.sub(r\"what's\", \"\", text)\n",
        "    #text               = re.sub(r\"What's\", \"\", text)\n",
        "    text                = re.sub(r\"\\'s\", \" \", text)\n",
        "    text                = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text                = re.sub(r\"can\\'t\", \"cannot \", text)\n",
        "    text                = re.sub(r\"n\\'t\", \" not \", text)\n",
        "    text                = re.sub(r\"i\\'m\", \"i am\", text)\n",
        "    text                = re.sub(r\" m \", \" am \", text)\n",
        "    text                = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text                = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text                = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text                = re.sub('[0-9]+', '', text)\n",
        "   \n",
        "    \n",
        "    \n",
        "    #if with_punct_sent==False:\n",
        "    #    pass\n",
        "        #text = ''.join([c for c in text if c not in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'])\n",
        "    #else: \n",
        "     #   text = ''.join([c for c in text if c not in '!.?'])\n",
        "        \n",
        "    # Optionally, remove stop words\n",
        "    if remove_stop_words:\n",
        "        text           = text.split()\n",
        "        text           = [w for w in text if not w in stop_words]\n",
        "        text           = \" \".join(text)\n",
        "\n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text           = text.split()\n",
        "        stemmer        = SnowballStemmer('english')\n",
        "        stemmed_words  = [stemmer.stem(word) for word in text]\n",
        "        text           = \" \".join(stemmed_words)\n",
        "   \n",
        "    ltr = text.split()\n",
        "    new_data = []\n",
        "    for i in ltr:\n",
        "        arr = str(i).split()\n",
        "        xx = \"\"\n",
        "        for j in arr:\n",
        "            j = str(j).lower()\n",
        "            if j[:4] == 'http' or j[:3] == 'www':\n",
        "                continue\n",
        "            xx += j + ' '\n",
        "        new_data.append(xx)\n",
        "    text = ''.join(new_data)\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rst_H6WQqrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Preparing the data without any punctuation')\n",
        "train_no_punct = train.copy()\n",
        "\n",
        "print('Train')\n",
        "train_no_punct.loc[:,'comment_text'] = train['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS90Dor4Q2QY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_no_punct.to_csv(data_dir + 'train_cleaned_no_punkt.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym4mgb80GFd_",
        "colab_type": "text"
      },
      "source": [
        "Save cleaned text as csv document in google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGFsYMVKGCqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sent_punct.to_csv('/content/drive/My Drive/BERT/BERT_Code_Input_Output/train_cleaned_sent_punkt.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}